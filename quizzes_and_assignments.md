سوالات کوییز و پروژه‌های LLM
فصل 1: موارد استفاده و چرخه عمر پروژه و آشنایی با Hugging Face
سوالات کوییز:

    تعامل با مدل‌های زبان بزرگ (LLM) با مدل‌های یادگیری ماشین سنتی متفاوت است. کار با LLMها شامل ورودی زبان طبیعی است که به عنوان _____ شناخته می‌شود و منجر به خروجی از مدل زبان بزرگ می‌شود که به عنوان _____ شناخته می‌شود.

        درخواست قابل تنظیم، Completion

        پرامپت، Completion

        درخواست پیش‌بینی، پاسخ پیش‌بینی

        پرامپت، LLM تنظیم‌شده

    مدل‌های زبان بزرگ (LLM) قادر به انجام چندین وظیفه هستند که از موارد استفاده مختلف پشتیبانی می‌کنند. کدام یک از وظایف زیر از مورد استفاده تبدیل کامنت‌های کد به کد قابل اجرا پشتیبانی می‌کند؟

        ترجمه

        بازیابی اطلاعات

        خلاصه‌سازی متن

        فراخوانی اقدامات از متن

    کدام کاربرد از LLM ها می‌تواند در یک متن اسم سازمان‌ها را برای ما مشخص کند؟

        استخراج موجودیت

        بازیابی اطلاعات

        خلاصه‌سازی متن

        فراخوانی اقدامات از متن

    کدام یک از موارد زیر، مزیت اصلی معماری "ترنسفورمر" (Transformer) نسبت به "RNN" (شبکه عصبی بازگشتی) در مدیریت وابستگی‌های طولانی‌مدت (Long-range Dependencies) در دنباله‌های داده است؟

        شبکه عصبی بازگشتی قادر به یادگیری گرامر و ساختار زبان بهتری هستند.

        شبکه عصبی بازگشتی از حافظه کاری (working memory) بزرگ‌تری برای ذخیره اطلاعات گذشته استفاده می‌کنند.

        ترنسفورمرها از مکانیسم خودتوجهی (Self-Attention) استفاده می‌کنند که به آن‌ها اجازه می‌دهد وابستگی بین هر دو توکن در دنباله را، بدون در نظر گرفتن فاصله‌ی آن‌ها، به طور مستقیم مدل‌سازی کنند.

        شبکه عصبی بازگشتی برای پردازش موازی (Parallel Processing) دنباله‌ها طراحی شده‌اند که باعث سرعت بیشتر آن‌ها می‌شود.

    مکانیزم توجه-به-خود (self-attention) که قدرت معماری ترنسفورمر را تامین می‌کند چیست؟

        توانایی ترنسفورمر برای تجزیه و تحلیل عملکرد خود و انجام تنظیمات بر اساس آن.

        مکانیزمی که به مدل اجازه می‌دهد در طول محاسبات بر روی بخش‌های مختلف دنباله ورودی تمرکز کند.

        معیاری برای سنجش میزان درک و تولید زبان انسان‌مانند توسط مدل.

        تکنیکی که برای بهبود قابلیت‌های تعمیم مدل با آموزش آن بر روی مجموعه داده‌های متنوع استفاده می‌شود.

    در پردازش زبان طبیعی (NLP) و با توجه به مدل‌های زبان بزرگ (LLM)، "توکن‌سازی" (Tokenization) به کدام فرآیند اشاره دارد؟

        فرآیند آموزش یک مدل زبان برای پیش‌بینی کلمه بعدی در یک جمله.

        روشی برای تولید جملات تصادفی و بی‌معنی از یک مدل.

        تبدیل یک دنباله از کاراکترها (مانند کلمات یا زیرکلمات) به واحدهای عددی که مدل می‌تواند آن‌ها را پردازش کند.

        فرآیند ارزیابی کیفیت خروجی یک مدل زبان.

    کدام یک از موارد زیر، انواع اصلی معماری‌های "ترنسفورمر" هستند که در ساخت مدل‌های زبان بزرگ (LLM) به کار می‌روند؟

        تنها انکودر (Encoder-Only)، تنها رمزگشا (Decoder-Only)، رمزگشا-رمزگشا (Decoder-Decoder)

        تنها رمزگشا (Decoder-Only)، تنها انکودر (Encoder-Only)، ترکیبی (Hybrid)

        تنها انکودر (Encoder-Only)، تنها رمزگشا (Decoder-Only)، انکودر-رمزگشا (Encoder-Decoder)

        تنها رمزگشا (Decoder-Only)، انکودر-رمزگشا (Encoder-Decoder)، رمزگشا-رمزگشا (Decoder-Decoder)

    شبکه‌های عصبی بازگشتی (RNN) برای وظایف هوش مصنوعی مولد بهتر از ترنسفورمرها هستند. آیا این درست است یا غلط؟

        درست

        غلط

    کدام یک از پارامترهای پیکربندی تولیدی زیر، مستقیماً بر "خلاقیت" یا "تصادفی بودن" خروجی LLM تأثیر می‌گذارد؟

        حداکثر توکن‌ها (Max Tokens)

        جریمه تکرار (Repetition Penalty)

        دما (Temperature)

        K-top

پروژه‌ها:

تمرین عملی 1-1:

    در اجرای کد با max_new_tokens=10 چه تفاوتی در خروجی مشاهده می‌کنید؟

    با فعال کردن do_sample=True و تغییر مقدار temperature، چه تغییراتی در متن تولیدشده می‌بینید؟

    به نظر شما کدام تنظیمات خروجی بهتری نسبت به خلاصه انسانی تولید کرده‌اند؟ چرا؟

تمرین عملی 1-2:

    گفت و گوی زیر را در نظر بگیرید:

        "علی: سلام، داداش! چطوری؟
        محمد: سلام علی! خوبم، تو چطوری؟
        علی: منم خوبم. این هفته قراره بریم فوتبال؟
        محمد: آره، می‌خوام برم. ساعت چند؟
        علی: ساعت ۵ تو پارک محل. بیا با هم بریم.
        محمد: اوکی، منتظرت می‌مونم.
        علی: بعد بازی می‌خوای بریم بستنی؟
        محمد: آره، خیلی خوب میشه!
        علی: پس شنبه همدیگه رو می‌بینیم.
        محمد: حتماً، خداحافظ!"

    الف) ابتدا به صورت zero-shot آن را خلاصه کنید.

    ب) سپس پرامپتی بنویسید که آن را به انگلیسی ترجمه کند.

    ج) (اختیاری) یک مدل دیگر نظیر mistral را لود کرده و همین دو تست را با آن مدل انجام دهید.

فصل 2: پیش‌آموزش LLM و قوانین مقیاس‌پذیری
سوالات کوییز:

    کدام معماری مدل مبتنی بر ترنسفورمر، هدفش حدس زدن یک توکن ماسک شده بر اساس دنباله قبلی توکن‌ها با ساخت نمایش‌های دوطرفه از دنباله ورودی است؟

        کدگذار خودکار (Autoencoder)

        خودهمبسته (Autoregressive)

        توالی به توالی (Sequence-to-sequence)

    کدام معماری مدل مبتنی بر ترانسفورمر برای وظیفه ترجمه متن مناسب است؟

        کدگذار خودکار (Autoencoder)

        خودهمبسته (Autoregressive)

        توالی به توالی (Sequence-to-sequence)

    آیا همیشه برای بهبود عملکرد مدل نیاز به افزایش اندازه مدل داریم؟

        درست

        غلط

    قوانین مقیاس‌بندی برای پیش‌آموزش مدل‌های زبان بزرگ، چندین جنبه را برای به حداکثر رساندن عملکرد مدل در مجموعه‌ای از محدودیت‌ها و گزینه‌های مقیاس‌بندی موجود در نظر می‌گیرند. کدام جایگزین‌های زیر باید برای مقیاس‌بندی هنگام انجام پیش‌آموزش مدل در نظر گرفته شوند؟

        اندازه بچ (Batch size): تعداد نمونه‌ها در هر تکرار

        اندازه مدل: تعداد پارامترها

        اندازه مجموعه داده: تعداد توکن‌ها

        بودجه محاسباتی: محدودیت‌های محاسباتی

    "می‌توانید موازی‌سازی داده‌ها را با موازی‌سازی مدل ترکیب کنید تا LLM‌ها را آموزش دهید." آیا این درست است یا غلط؟

        درست

        غلط

فصل 3: تنظیم دقیق (Fine-tuning) مدل‌های LLM با دستورالعمل‌ها و ارزیابی آن‌ها
سوالات کوییز:

    جاهای خالی را پر کنید: _____________ شامل استفاده از مثال‌های متعدد پرامپت- Completion به عنوان مجموعه داده آموزشی برچسب‌گذاری شده برای ادامه آموزش مدل با به‌روزرسانی وزن‌های آن است. این با _____________ متفاوت است که در آن شما مثال‌های پرامپت- Completion را در طول استنتاج (inference) ارائه می‌دهید.

        یادگیری درون متنی (In-context learning)، تنظیم دقیق دستوری (Instruction fine-tuning)

        مهندسی پرامپت، پیش‌آموزش (Pre-training)

        پیش‌آموزش (Pre-training)، تنظیم دقیق دستوری (Instruction fine-tuning)

        تنظیم دقیق دستوری (Instruction fine-tuning)، یادگیری درون متنی (In-context learning)

    فاین تیون کردن یک مدل بر روی یک وظیفه واحد می‌تواند عملکرد مدل را به طور خاص در آن وظیفه بهبود بخشد؛ با این حال، می‌تواند عملکرد وظایف دیگر را نیز به عنوان یک عارضه جانبی کاهش دهد. این پدیده به عنوان:

        از دست دادن فاجعه‌بار (Catastrophic loss)

        فراموشی فاجعه‌بار (Catastrophic forgetting)

        سوگیری دستوری (Instruction bias)

        سمیت مدل (Model toxicity)

    کدام یک از گزاره‌های زیر در مورد تنظیم دقیق چندوظیفه‌ای (multi-task finetuning) صحیح است؟ (همه موارد قابل اجرا را انتخاب کنید)

        فاین تیون کردن چندوظیفه‌ای ممکن است منجر به استنتاج کندتر شود.

        فاین تیون چندوظیفه‌ای می‌تواند به جلوگیری از فراموشی فاجعه‌بار کمک کند.

        مدل FLAN-T5 با فاین تیون چندوظیفه‌ای آموزش دیده است.

        فاین تیون چندوظیفه‌ای به مدل‌های جداگانه برای هر وظیفه در حال انجام نیاز دارد.

    کدام معیار ارزیابی زیر بر دقت در تطابق خروجی تولید شده با متن مرجع تمرکز دارد و برای ترجمه متن استفاده می‌شود؟

        HELM

        BLEU

        ROUGE-2

        ROUGE-1

    کدام معیار ارزیابی زیر برای خلاصه سازی متن استفاده می‌شود؟

        HELM

        BLEU

        ROUGE

        Accuracy

پروژه‌ها:

تمرین عملی 3-1:

    با 10 جمله انگلیسی به فارسی که در فایل اصلی آمده، یک دیتاست کوچک ساخته و با پرامپت نویسی و ICL (بدون فاین تیون) از یک مدل دلخواه استفاده کرده تا انگلیسی را به فارسی ترجمه کند. خروجی را با BlEU score ارزیابی کنید.

    یکبار هم این جملات را با گوگل ترنسلیت ترجمه کرده و BLEU ها را باهم مقایسه کنید.

فصل 4: فاین‌تیون کردن کارآمد پارامترها (PEFT)
سوالات کوییز:

    LLM‌های کوچک‌تر می‌توانند با استنتاج تک‌نمونه‌ای (one-shot) و چندنمونه‌ای (few-shot) مشکل داشته باشند. آیا این درست است یا غلط؟

        درست

        غلط

    کدام یک از موارد زیر روش‌های تنظیم دقیق کارآمد پارامتر (PEFT) هستند؟ (همه موارد قابل اجرا را انتخاب کنید)

        بازپارامترسازی (Reparametrization)

        کاهشی (Subtractive)

        انتخابی (Selective)

        افزایشی (Additive)

    روش‌های PEFT می‌توانند حافظه مورد نیاز برای تنظیم دقیق را به طرز چشمگیری کاهش دهند، گاهی اوقات تا فقط 12-20 ٪ از حافظه مورد نیاز برای فاین تیون کامل. آیا این درست است یا غلط؟

        درست

        غلط

    کدام یک از گزاره‌های زیر به بهترین وجه نحوه کار LoRA را توضیح می‌دهد؟

        روش LoRA وزن‌ها را به دو ماتریس با رتبه کوچک‌تر تجزیه می‌کند و به جای وزن‌های کامل مدل، آن‌ها را آموزش می‌دهد.

        روش LoRA تمام وزن‌ها را در لایه‌های مدل اصلی فریز می‌کند و مولفه‌های جدیدی را معرفی می‌کند که بر روی داده‌های جدید آموزش داده می‌شوند.

        روش LoRA یک نسخه کوچک‌تر و تقطیر شده از LLM از پیش آموزش‌دیده را آموزش می‌دهد تا اندازه مدل را کاهش دهد.

        روش LoRA هدف اصلی پیش‌آموزش را بر روی داده‌های جدید ادامه می‌دهد تا وزن‌های مدل اصلی را به‌روزرسانی کند.

    سافت پرامپت (soft prompt) در زمینه LLM چیست؟

        مجموعه‌ای از توکن‌های قابل آموزش که به یک پرامپت اضافه می‌شوند و مقادیر آن‌ها در طول آموزش اضافی برای بهبود عملکرد در وظایف خاص به‌روزرسانی می‌شوند.

        یک متن ورودی سخت و صریح که به عنوان نقطه شروعی برای تولید مدل عمل می‌کند.

        تکنیکی برای محدود کردن خلاقیت مدل و اعمال الگوهای خروجی خاص.

        روشی برای کنترل رفتار مدل با تنظیم نرخ یادگیری در طول آموزش.

    تنظیم پرامپت (Prompt Tuning) تکنیکی است که برای تنظیم تمام هایپرپارامترهای یک مدل زبان استفاده می‌شود. آیا این درست است یا غلط؟

        درست

        غلط

پروژه‌ها:

تمرین عملی 4-1:

    مثال ترجمه ماشینی (انگلیسی به فارسی) نوت بوک 6 را با prompt tuning انجام دهید.

    سپس ببینید که آیا معیار BLEU ترجمه آن ده جمله تمرین عملی 3-1، نسبت به قبل فاین تیون بالاتر رفته یا خیر؟!

مینی‌پروژه 1: خلاصه‌سازی کوتاه خبر با PEFT روی مدل مولد

    صورت تمرین: با استفاده از روش PEFT (مانند QLoRA)، یک مدل زبانی مولد (مثل TinyLlama یا Gemma2 یا ...) را روی دیتاست CNN/DailyMail فاین‌تیون کرده و آموزش دهید تا بتواند از یک مقاله خبری، یک یا چند جمله خلاصه (highlights) تولید کند.

    خروجی مورد انتظار: نوت‌بوک Colab با کدهای کامل، چند مثال از ورودی/خروجی مدل، گزارش متنی (یا PDF اختیاری) با تحلیل.

مینی پروژه 2: شناسایی موجودیت‌های نامدار فارسی با استفاده از PEFT

    هدف: آموزش یک مدل زبانی دلخواه از پیش آموزش‌دیده مانند HooshvareLab/bert-fa-base-uncased با استفاده از روش PEFT بر روی دیتاست NER فارسی. هدف، تشخیص توکن‌های موجودیت‌های نامدار با برچسب 1 و توکن‌های دیگر با برچسب 0 است.

    دیتاست: دیتاست NER فارسی از لینک https://github.com/Mostafa-Modaberi.

    تحویل پروژه: کد کامل پروژه در یک نوت‌بوک Jupyter، یک گزارش کوتاه با توضیحات، نتایج ارزیابی و تحلیل.

فصل 5: بازخورد انسانی و الگوریتم‌های پیشرفته یادگیری تقویتی
سوالات کوییز:

    جاهای خالی را پر کنید: هنگام فاین تیون کردن یک مدل زبان بزرگ با بازخورد انسانی، عملی که agent یا عامل (در این مورد LLM) انجام می‌دهد _____________ و فضای عمل _____________ است.

        تولید توکن بعدی، مجموعه تمام واژگان (توکن‌ها)

        پردازش پرامپت، پنجره متنی (context window).

        محاسبه توزیع احتمال، وزن‌های مدل LLM.

        تولید توکن بعدی، پنجره متنی (context window).

    "Proximal" در بهینه‌سازی سیاست پروگزیمال (Proximal Policy Optimization) به چه چیزی اشاره دارد؟

        توانایی الگوریتم در اداره سیاست‌های پروگزیمال

        نزدیکی الگوریتم به سیاست بهینه

        استفاده از الگوریتم گرادیان نزولی پروگزیمال

        محدودیتی که فاصله بین سیاست جدید و قدیم را محدود می‌کند.

    در یادگیری تقویتی، به ویژه با الگوریتم بهینه‌سازی سیاست پروگزیمال (PPO)، نقش واگرایی KL چیست؟ (همه موارد قابل اجرا را انتخاب کنید)

        واگرایی KL تفاوت بین دو توزیع احتمال را اندازه می‌گیرد.

        واگرایی KL برای اعمال محدودیتی استفاده می‌شود که میزان به‌روزرسانی وزن‌های LLM را محدود می‌کند.

        واگرایی KL به‌روزرسانی‌های بزرگ وزن‌های LLM را تشویق می‌کند تا تفاوت‌ها با مدل اصلی را افزایش دهد.

        واگرایی KL برای آموزش مدل پاداش با امتیازدهی تفاوت تکمیل‌های جدید از موارد اصلی با برچسب انسانی استفاده می‌شود.

    هدف اصلی روش Direct Preference Optimization (DPO) در تربیت مدل‌های زبان بزرگ چیست؟

        افزایش سرعت آموزش مدل در مراحل اولیه

        بهینه‌سازی مدل براساس ترجیحات انسانی بدون نیاز به مدل پاداش

        کاهش تعداد پارامترهای مدل زبان بزرگ

        جایگزینی کامل مرحله پیش‌پردازش داده‌ها در RLHF

    در روش DPO، چه چیزی به عنوان داده آموزشی استفاده می‌شود؟

        یک توکن و احتمال آن در توزیع نهایی

        جفت پاسخِ برتر و ضعیف (preferred vs. dispreferred)

        فقط پاسخ‌های دارای امتیاز عددی بالا

        خلاصه‌های متنی تولیدشده توسط مدل زبان

    تفاوت کلیدی GRPO با DPO در چیست؟

        GRPO از داده‌های بدون برچسب استفاده می‌کند

        GRPO صرفاً در مدل‌های رمزگشا کاربرد دارد

        GRPO به یک تابع پاداش عددی نیاز دارد، در حالی که DPO از مقایسه جفتی استفاده می‌کند

        GRPO برای مدل‌های تصویری طراحی شده است

    در روش GRPO، چه نوع داده‌ای برای آموزش استفاده می‌شود؟

        جفت پاسخِ خوب و بد (preferred vs. dispreferred)

        داده‌های ساختارمند همراه با برچسب

        پاسخ‌هایی با امتیاز عددی (scalar reward)

        خلاصه‌های تولیدشده توسط مدل و بررسی شده توسط انسان

    در فرآیند تربیت مدل‌های زبان با بازخورد انسانی (Human Feedback)، کدام گزینه به‌درستی تفاوت میان روش‌های PPO، DPO و GRPO را نشان می‌دهد؟

        PPO و GRPO هر دو از امتیاز عددی (scalar reward) برای بهینه‌سازی استفاده می‌کنند، در حالی که DPO تنها به داده‌های بدون برچسب نیاز دارد.

        DPO و GRPO به مدل پاداش نیاز دارند، اما PPO مستقیماً از ترجیحات انسانی استفاده می‌کند.

        PPO از مدل پاداش و الگوریتم تقویتی استفاده می‌کند، DPO از جفت پاسخ ترجیحی بدون نیاز به مدل پاداش بهره می‌برد، و GRPO مستقیماً پاسخ‌ها را با پاداش عددی بهینه می‌کند بدون نیاز به الگوریتم RL سنتی.

        هر سه روش از داده‌های مقایسه‌ای استفاده می‌کنند، اما تنها GRPO از الگوریتم تقویتی بهره می‌گیرد.

    کدام یک از موارد زیر در مورد هوش مصنوعی قانون‌مند (Constitutional AI) صحیح است؟ (همه موارد قابل اجرا را انتخاب کنید)

        روش Red Teaming فرآیند استخراج پاسخ‌های نامطلوب از طریق تعامل با یک مدل است.

        برای هوش مصنوعی قانون‌مند، ارائه بازخورد انسانی برای راهنمایی اصلاحات ضروری است.

        برای به دست آوردن پاسخ‌های بازبینی شده برای پرامپت‌های بالقوه مضر، باید فرآیند نقد و بازبینی را طی کنیم.

پروژه‌ها:

    با یک مدل دلخواه، مدل را با روش DPO روی این دیتاست آموزش دهید: https://huggingface.co/datasets/Intel/orca_dpo_pairs/viewer/default/train?views%5B%5D=train&row=31

    اگر محدودیت سخت افزاری دارید می‌توانید با یک زیر مجموعه مثلا 1000 نمونه‌ای آموزش دهید.

فصل 6: بهینه‌سازی مدل‌ها برای استقرار (Deploy)
سوالات کوییز:

    در روش تقطیر دانش (Knowledge Distillation) در یادگیری ماشین، کدام گزینه به‌درستی فرآیند و هدف اصلی آن را توصیف می‌کند؟

        آموزش یک مدل بزرگ‌تر برای تقلید خروجی‌های یک مدل کوچک‌تر، به‌منظور افزایش دقت و پیچیدگی مدل

        انتقال دانش از مدل دانش‌آموز (student) به مدل معلم (teacher) از طریق بهینه‌سازی همزمان

        آموزش یک مدل کوچک‌تر (student) برای تقلید رفتار مدل بزرگ‌تر (teacher) با هدف کاهش هزینه محاسباتی در حالی که دقت حفظ شود

        استفاده از مدل‌های بدون نظارت برای آموزش مدل‌های نظارت‌شده در مقیاس بزرگ

    در بهینه‌سازی مدل‌های زبان بزرگ، کدام گزینه به‌درستی تفاوت میان "کوانتیزیشن" و "هرس مدل" را بیان می‌کند؟

        کوانتیزیشن باعث کاهش پارامترهای مدل می‌شود، در حالی که هرس مدل فقط سرعت inference را افزایش می‌دهد.

        هر دو روش تنها در مرحله پیش‌پردازش داده‌ها به کار می‌روند و تاثیری بر ساختار مدل ندارند.

        کوانتیزیشن با کاهش دقت عددی پارامترها (مثلاً از float32 به int8) حجم مدل را کاهش می‌دهد، در حالی که هرس مدل با حذف برخی اتصالات یا نرون‌ها، ساختار مدل را کوچک‌تر می‌کند.

        هرس مدل به‌صورت دینامیک در زمان inference انجام می‌شود، اما کوانتیزیشن تنها در مرحله آموزش مؤثر است.

فصل 7: ساخت اپلیکیشن‌های هوشمند با LLM
سوالات کوییز:

    بازیابی تولید افزوده (RAG) چگونه مدل‌های مبتنی بر تولید را بهبود می‌بخشد؟

        با اعمال تکنیک‌های یادگیری تقویتی برای افزایش تکمیل‌ها.

        با در دسترس قرار دادن دانش خارجی برای مدل.

        با افزایش اندازه داده‌های آموزشی.

        با بهینه‌سازی معماری مدل برای تولید تکمیل‌های واقعی.

    چگونه گنجاندن تکنیک‌های بازیابی اطلاعات می‌تواند کاربرد LLM شما را بهبود بخشد؟ (همه موارد قابل اجرا را انتخاب کنید)

        غلبه بر محدودیت‌های دانشی (Knowledge Cut-offs)

        سرعت آموزش سریع‌تر در مقایسه با مدل‌های سنتی.

        کاهش ردپای حافظه برای مدل.

        بهبود ارتباط و دقت پاسخ‌ها.

    در معماری RAG (Retrieval-Augmented Generation)، نقش اصلی ماژول بازیابی (Retriever) چیست؟

        تولید پاسخ نهایی با استفاده از مدل زبان

        تبدیل سوال به امبدینگ عددی

        یافتن تکه‌های مرتبط از منابع خارجی مانند PDF

        ارزیابی کیفیت پاسخ تولیدشده توسط مدل

    در فرآیند آماده‌سازی RAG با LangChain، امبدینگ‌ها چه نقشی دارند؟

        بهینه‌سازی حافظه مدل زبان برای سرعت بیشتر

        تبدیل متن به بردار عددی جهت جستجوی معنایی

        تنظیم وزن‌های لایه خروجی مدل

        مشخص کردن طول توکن‌های تولیدی توسط LLM

    چرا فایل PDF برای RAG باید "تکه‌بندی (chunking)" شود؟

        برای کاهش حجم فایل

        برای فشرده‌سازی اطلاعات جهت ذخیره‌سازی بهتر

        برای تقسیم متن به بخش‌های قابل مدیریت جهت بازیابی دقیق‌تر

        برای سازگاری با مدل‌های زبانی غیرمتنی

    در LangChain، کدام ترکیب به‌درستی یک pipeline ساده RAG را توصیف می‌کند؟

        LLM → Retriever → Embeddings

        PDF → LLM → Chunking

        PDF → Chunking → Embeddings → Retriever → LLM

        Retriever → Vector DB → LLM → Summarizer

    تعریف صحیح مدل‌های زبان برنامه‌محور (PAL) چیست؟

        مدل‌هایی که وظایف محاسباتی را به برنامه‌های دیگر واگذار می‌کنند.

        مدل‌هایی که قابلیت‌های ترجمه زبان و کدنویسی را یکپارچه می‌کنند.

        مدل‌هایی که از طریق رابط‌های زبان طبیعی به برنامه‌نویسان در نوشتن کد کمک می‌کنند.

        مدل‌هایی که ترجمه خودکار زبان‌های برنامه‌نویسی به زبان‌های انسانی را امکان‌پذیر می‌کنند.

    کدام یک از موارد زیر به بهترین وجه تمرکز اصلی ReAct را توصیف می‌کند؟

        بررسی قابلیت‌های استدلال در LLM‌ها از طریق پرامپت زنجیره‌ای از افکار.

        مطالعه موضوعات جداگانه استدلال و عمل در LLM‌ها.

        بررسی تولید طرح عمل در LLM‌ها.

        افزایش درک زبان و تصمیم‌گیری در LLM‌ها.

پروژه‌ها:

    نوت بوک 17 را با یک PDF دلخواه دیگر تست کرده و حداقل 3 سوال از آن بپرسید.

    سپس با جست و جوو و پیدا کردن مدل استخراج امبدینگ (retriever) بهتر از fabert آن را عوض کرده و سعی کنید کیفیت پاسخگویی را بالا ببرید.

